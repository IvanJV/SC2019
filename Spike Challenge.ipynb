{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spike Challenge Octubre 2019\n",
    "## Ivan Jara Varela"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importadas las librerías a utilizar, comenzamos:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Número 1\n",
    "\n",
    "Descarga del archivo csv para crear el dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_gbq('SELECT * FROM public.caudal_extra_min',project_id='spikelab') \n",
    "## Para base de datos en BigQuery (no tenía acceso)\n",
    "\n",
    "df = pd.read_csv('caudal_extra.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Número 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hay varias filas con datos faltantes, porque no todas las cuencas tienen estaciones de medición de precipitación o temperatura."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows_nan_t = df['temp_max_promedio'].isna().sum()\n",
    "num_rows_nan_p = df['precip_promedio'].isna().sum()\n",
    "print('Hay ',num_rows_nan_t,' filas sin registro de temperatura')\n",
    "print('Hay ',num_rows_nan_p,' filas sin registro de precipitación')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver un resumen de las tres principales variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[['caudal','temp_max_promedio','precip_promedio']].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También podemos ver si la distribución de las variables es asimétrica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[['caudal','temp_max_promedio','precip_promedio']].skew())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las mediciones de caudal y precipitación se concentran más a la derecha (mediciones de caudal mayores al promedio) y las de temperatura a la izquierda (menores al promedio), considerando una distribución normal.\n",
    "\n",
    "Hay dos columnas iguales, gauge_id y codigo_estacion, por lo que se elimina una"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(set(df['gauge_id'] == df['codigo_estacion']))\n",
    "df.drop(columns='gauge_id',inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hay columnas inútiles, como institución y fuente, porque tienen el mismo valor en todas las filas (mediciones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Hay ',len(set(df['institucion'])),' valor para todas las filas de la columna institución')\n",
    "print('Hay ',len(set(df['fuente'])),' valor para todas las filas de la columna fuente')\n",
    "df.drop(columns=['institucion','fuente'],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También se puede saber cuantas cuencas y estaciones hay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cuencas = len(set(df['codigo_cuenca']))\n",
    "num_estaciones = len(set(df['codigo_estacion']))\n",
    "print('Hay ',num_cuencas,' cuencas, monitoreadas en ',num_estaciones,' estaciones')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, dejamos solo las variables numéricas, transformamos la columna fecha a formato datetime y eliminamos el dataframe original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff = df[['codigo_estacion',\n",
    "          'codigo_cuenca',\n",
    "          'cantidad_observaciones',\n",
    "          'altura',\n",
    "          'latitud',\n",
    "          'longitud',\n",
    "          'fecha',\n",
    "          'caudal',\n",
    "          'precip_promedio',\n",
    "          'temp_max_promedio']]\n",
    "dff = df\n",
    "dff['fecha'] = pd.to_datetime(dff['fecha'])\n",
    "\n",
    "del df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Número 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las estaciones, mientras existen, tienen registros de caudal en todo el periodo; para la temperatura y la precipitación no es el caso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_plot_una_estacion(codigo_estacion,columna,fecha_min,fecha_max):\n",
    "    df1 = dff[dff['codigo_estacion'] == codigo_estacion]\n",
    "    df1.set_index('fecha',inplace=True)\n",
    "    df1 = df1[[columna]]\n",
    "    df1 = df1[fecha_min:fecha_max]\n",
    "    return df1.plot(title = 'Código estación: ' + str(codigo_estacion))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación hay un ejemplo de la implementación de la función."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_plot_una_estacion(4540001,'caudal','1965-01-06','1976-02-13')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_plot_estaciones_varias_columnas(codigo_estacion,columnas,fecha_min,fecha_max):\n",
    "    df1 = dff[dff['codigo_estacion'] == codigo_estacion]\n",
    "    df1.set_index('fecha',inplace=True)\n",
    "    df1 = df1[columnas]\n",
    "    df1 = 100*(df1 - df1.min())/(df1.max()- df1.min())\n",
    "    df1 = df1[fecha_min:fecha_max]\n",
    "    return df1.plot(title= 'est: ' + str(codigo_estacion) + ', normalizado con min/max')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un ejemplo de la implementación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_plot_estaciones_varias_columnas(4540001,['caudal','precip_promedio','temp_max_promedio'],'1980-01-06','1985-02-13')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A modo de \"prueba\" se puede ver como la temperatura varía al pasar el año, lo que indica que parece estar bien."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Número 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizando una muy buena forma de transformar el formato datetime a meteorological season (verano,invierno,primavera y otoño), adaptada desde [aquí](https://stackoverflow.com/questions/44124436/python-datetime-to-season)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff['season'] = [(dt.month%12 + 3)//3 for dt in dff['fecha']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se crea una función para calcular el percentil 95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def per95(x):\n",
    "    return np.percentile(x,95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se crea un nuevo dataframe con pivot_table, donde los índices sean codigo_estacion y season, y los valores el percentil 95 de las 3 variables  en estudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_q = pd.pivot_table(dff,index=['codigo_estacion','season'],values=['caudal','precip_promedio','temp_max_promedio'],\\\n",
    "                      aggfunc=per95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se crea una función para asignar 1 si un número es mayor, y 0 si es menor a otro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mm(x,y):\n",
    "    if (x >= y):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, se aplica la función anterior comparando el dataframe df_q y dff, creando las columnas requeridas, para luego eliminar df_q y los arrays temporales. Este toma 5 minutos en mi computador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_xtrem = []\n",
    "p_xtrem = []\n",
    "t_xtrem = []\n",
    "for dc,dp,dt,de,ds in zip(dff['caudal'],dff['precip_promedio'],\\\n",
    "                          dff['temp_max_promedio'],\\\n",
    "                          dff['codigo_estacion'],dff['season']):\n",
    "    xtrem = df_q.loc[(de,ds)]\n",
    "    c_xtrem.append(mm(dc,xtrem[0]))\n",
    "    p_xtrem.append(mm(dp,xtrem[1]))\n",
    "    t_xtrem.append(mm(dt,xtrem[2]))\n",
    "\n",
    "\n",
    "dff['caudal_extremo'] = c_xtrem\n",
    "dff['precip_extremo'] = p_xtrem\n",
    "dff['temp_extremo'] = t_xtrem\n",
    "del df_q, c_xtrem, p_xtrem, t_xtrem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff[['codigo_estacion','season','caudal_extremo','precip_extremo','temp_extremo']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "con 1:verano 2:otoño 3:invierno y 4:primavera"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El filtro por el percentil 95 es una buena manera de capturar los eventos extremos. Otra forma sería aprovechar la aleatoriedad de los datos; se me ocurre que la distribución de eventos extremos en el tiempo tiene más entropía, pero por ahora no sé como calcular algo así."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Número 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero, se crea un nuevo dataframe, y utilizamos nuevamente pivot_tablet para hacer el aggregate de eventos extremos de caudal por cuenca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_caudal = dff.pivot_table(index='codigo_cuenca',values='caudal_extremo',aggfunc=np.sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego, se grafica el número de ocurrencias de eventos extremos de caudal por cuenca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_caudal.plot.bar(title='N° de ocurrencias por cuenca')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver como la cantidad de eventos extremos de caudal entre algunas cuencas difiere en 1 orden de magnitud (10 veces más grandes) en algunos casos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Número 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se crea una nueva columna, 'year', y un nuevo dataframe, df6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = [dt.year for dt in dff['fecha']]\n",
    "dff['year'] = years\n",
    "df6 = dff.drop(columns='fecha')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por última vez, se utiliza pivot_table para efectuar el aggregate de las variables, esta vez con la siguiente función, que entrega el porcentaje de eventos extremos de cada variable en cada año"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perc100(x):\n",
    "    return 100*np.mean(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df6 = pd.pivot_table(dff,index='year',values=['caudal_extremo','precip_extremo','temp_extremo'],aggfunc=perc100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, se grafica y se eliminan las variables temporales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df6.plot(title = 'Eventos extremos por año') \n",
    "del years, df6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solo observando el gráfico anterior, los eventos de precipitación extrema han disminuido, los de caudal no parecen mostrar una tendencia y los de temperatura parecen subir. Considerar por separado cuencas, zonas geográficas u otras funciones de aggregate quizás muestren mas claramente alguna tendencia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Número 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se crean las variables 'month' y 'day', y se eliminan las filas que contengan valores missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff['month'] = [dt.month for dt in dff['fecha']]\n",
    "dff['day'] = [dt.day for dt in dff['fecha']]\n",
    "dff.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se fija el tamaño del dataset de prueba en un 25%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_s = 0.25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se divide el dataset en features(datax) y objetivo(datay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datax = dff[['codigo_estacion',\n",
    "          'codigo_cuenca',\n",
    "          'cantidad_observaciones',\n",
    "          'altura',\n",
    "          'latitud',\n",
    "          'longitud',\n",
    "          'year',\n",
    "          'month',\n",
    "          'day',\n",
    "          'caudal',   \n",
    "          'precip_promedio',\n",
    "          'temp_max_promedio']]\n",
    "\n",
    "\n",
    "datay = dff[['caudal_extremo']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se divide entre los train y test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test = train_test_split(datax,datay,test_size=test_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se entrenará un modelo K-Nearest Neighbors (KNN), con un radio de k=15, y ponderado por la distancia. El entrenamiento me tomó alrededor de 2 minutos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=15,weights='distance')\n",
    "knn.fit(x_train,y_train.values.ravel())\n",
    "y_pred = knn.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intenté ocupar todos los datos disponibles originales. Propongo utilizar las 11 variables para predecir caudal extremo en el futuro, o sea, simplemente ingresar los 11 valores requeridos del día a pronosticar. Como lo entiendo, más que predecir condiciones futuras, el modelo aprende los patrones cíclicos y la tendencia creciente o decreciente de la variable caudal extremo. \n",
    "\n",
    "KNN funciona en base a distancias euclidianas, por lo que en principio no hay restricciones numéricas para las features. Debería funcionar bien con datos razonables y dentro de las variaciones convencionales (no utilizar 60°C en temperatura, por ejemplo)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Número 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizando la métrica más sencilla, que es la comparación entre los datos predichos y el dataset de prueba apartado en el punto anterior, se tiene que:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('El modelo tiene una performance de ', round(metrics.accuracy_score(y_test, y_pred)*100,4),'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para KNN no conozco una manera directa de hacer análisis de sensibilidad, pero hay variables que quizás esten entregando la misma información (redundantes), como la cuenca, estación y las coordenadas.\n",
    "\n",
    "Quizás sirve \"apagar\" una de las features, y entrenar el modelo para ver como afecta en la metrica utilizada para medir performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A pesar de lo anterior, el entrenamiento no tarda mucho, y parece entregar buenos resultados. Es un buen modelo para comenzar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No comprendo la pregunta; en los modelos que conozco (y este en particular) creo que no se puede elegir exactamente qué datos se utilizaran para entrenar el modelo. Si la pregunta consiste en ocupar el 70% del dataset para entrenar, entonces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_s = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test = train_test_split(datax,datay,test_size=test_s)\n",
    "\n",
    "knn1 = KNeighborsClassifier(n_neighbors=15,weights='distance')\n",
    "knn1.fit(x_train,y_train.values.ravel())\n",
    "y_pred = knn1.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('El modelo tiene una performance de ', round(metrics.accuracy_score(y_test, y_pred)*100,4),'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Me parece que sigue siendo útil, porque es muy similar al resultado anterior."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
